from __future__ import annotations

import asyncio
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, Optional

from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

# —Ç–≤–æ–∏ –∏–º–ø–æ—Ä—Ç—ã –ø–æ–¥ –ø—Ä–æ–µ–∫—Ç
from dictapp.db import AsyncSessionMaker
from dictapp.models import Entry


# =========================
# Helpers: detect line types
# =========================

# CJK Unified + ExtA + Compatibility
_CJK_RE = re.compile(r"[\u3400-\u4DBF\u4E00-\u9FFF\uF900-\uFAFF]")

_PINYIN_LIKE_RE = re.compile(
    r"^[A-Za-z√Ä-√ñ√ò-√∂√∏-√øƒÄ-≈æ«ç-«ê«ë-«î«ï-«ú«û-«ü«†-«°«¢-«£«¶-«ß«®-«©«™-«´«¨-«≠«Æ-«Ø«∞-«≥«¥-«µ«∏-«π«∫-«ª«º-«Ω«æ-«ø\s'¬∑-]+$"
)

def has_cjk(s: str) -> bool:
    return bool(_CJK_RE.search(s))


def looks_like_pinyin(s: str) -> bool:
    s = s.strip()
    if not s:
        return False
    if has_cjk(s):
        return False
    # —á–∞—Å—Ç–æ –º—É—Å–æ—Ä —Ç–∏–ø–∞ "‚Äî"
    if len(s) > 120:
        return False
    # –µ—Å–ª–∏ –µ—Å—Ç—å –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ ‚Äî —ç—Ç–æ —Ç–æ—á–Ω–æ –Ω–µ –ø–∏–Ω—å–∏–Ω—å
    if re.search(r"[–ê-–Ø–∞-—è–Å—ë]", s):
        return False
    return bool(_PINYIN_LIKE_RE.match(s))


# =========================
# DSL cleanup
# =========================

_TAG_RE = re.compile(r"\[/?[A-Za-z0-9]+\]")      # [m1], [/m], [i], [/i], [p], [/p], [ref]...
_BRACE_RE = re.compile(r"\{[^}]*\}")            # {....}
_INCLUDE_RE = re.compile(r'#INCLUDE\s+"([^"]+)"')

def clean_dsl_text(text_: str) -> str:
    """
    –î–µ–ª–∞–µ—Ç —Ç–µ–∫—Å—Ç —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–º:
    - —É–±–∏—Ä–∞–µ—Ç DSL-—Ç–µ–≥–∏ [m1], [i], [/i], [p], [ref]...
    - —É–±–∏—Ä–∞–µ—Ç —Ñ–∏–≥—É—Ä–Ω—ã–µ {...}
    - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã/–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    """
    t = text_.replace("\u00A0", " ")
    t = _TAG_RE.sub("", t)
    t = _BRACE_RE.sub("", t)
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()


# =========================
# Universal DSL reader (headword + body) with #INCLUDE
# =========================

def iter_dsl_articles(path: Path, encoding: str = "utf-16") -> Iterator[tuple[str, str]]:
    """
    –ß–∏—Ç–∞–µ—Ç DSL –∫–∞–∫ –ø–∞—Ä—ã (headword, body_text).
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç #INCLUDE "file.dsl".

    –§–æ—Ä–º–∞—Ç —Å—Ç–∞—Ç—å–∏ DSL:
      headword (—Å—Ç—Ä–æ–∫–∞ –±–µ–∑ –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤/—Ç–∞–±–æ–≤)
      body (—Å—Ç—Ä–æ–∫–∏ —Å –≤–µ–¥—É—â–∏–º –ø—Ä–æ–±–µ–ª–æ–º/—Ç–∞–±–æ–º)
    """
    def walk_file(p: Path) -> Iterator[str]:
        with p.open("r", encoding=encoding, errors="replace") as f:
            for raw in f:
                line = raw.rstrip("\n").rstrip("\r")
                m = _INCLUDE_RE.match(line.strip())
                if m:
                    inc = (p.parent / m.group(1)).resolve()
                    if inc.exists():
                        yield from walk_file(inc)
                    continue
                yield line

    head: Optional[str] = None
    body_lines: list[str] = []

    def flush() -> Optional[tuple[str, str]]:
        nonlocal head, body_lines
        if head is None:
            return None
        body = "\n".join(body_lines).strip()
        out = (head.strip(), body)
        head = None
        body_lines = []
        return out

    for line in walk_file(path):
        if not line:
            if head is not None:
                body_lines.append("")
            continue

        st = line.strip()
        if not st:
            if head is not None:
                body_lines.append("")
            continue

        # –¥–∏—Ä–µ–∫—Ç–∏–≤—ã
        if st.startswith("#"):
            continue

        # –Ω–æ–≤–∞—è —Å—Ç–∞—Ç—å—è ‚Äî —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤/—Ç–∞–±–æ–≤
        if line[0] not in (" ", "\t"):
            prev = flush()
            if prev:
                yield prev
            head = st
        else:
            if head is None:
                continue
            body_lines.append(st)

    prev = flush()
    if prev:
        yield prev


# =========================
# Extractors for BRUKS
# =========================

def extract_first_hanzi(text_: str) -> Optional[str]:
    m = _CJK_RE.search(text_)
    if not m:
        return None

    start = m.start()
    end = m.start()

    while start > 0 and has_cjk(text_[start - 1]):
        start -= 1
    while end < len(text_) and has_cjk(text_[end]):
        end += 1

    hanzi = text_[start:end].strip()
    return hanzi if hanzi else None


def extract_first_pinyin(text_: str) -> Optional[str]:
    for line in text_.splitlines():
        s = line.strip()
        if looks_like_pinyin(s):
            return s
    return None


# =========================
# Parsed entry
# =========================

@dataclass
class ParsedEntry:
    hanzi: str
    pinyin: Optional[str]
    ru: str


def iter_entries_for_file(path: Path, encoding: str = "utf-16") -> Iterator[ParsedEntry]:
    """
    dabkrs_*.dsl (CN->RU):
      headword = hanzi
      pinyin = –ø–µ—Ä–≤–∞—è –ø–∏–Ω—å–∏–Ω—å-–ø–æ–¥–æ–±–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ body
      ru = body

    dabruks.dsl (RU->CN):
      headword = ru
      hanzi/pinyin –ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å –∏–∑ body
      ru –∫–ª–∞–¥—ë–º –∫–∞–∫ headword (—á—Ç–æ–±—ã —Ä—É—Å—Å–∫–∏–µ –ø—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞ —Ä–µ–∞–ª—å–Ω–æ –∏—Å–∫–∞–ª–∏—Å—å)
    """
    name = path.name.lower()
    is_bruks = "bruks" in name

    for head, body in iter_dsl_articles(path, encoding=encoding):
        body_clean = clean_dsl_text(body)

        if not is_bruks:
            hanzi = head.strip()
            if not hanzi:
                continue
            pinyin = extract_first_pinyin(body_clean)
            ru = body_clean
            if ru:
                yield ParsedEntry(hanzi=hanzi, pinyin=pinyin, ru=ru)
        else:
            ru_head = head.strip()
            if not ru_head:
                continue
            hanzi = extract_first_hanzi(body_clean) or "-"
            pinyin = extract_first_pinyin(body_clean)
            # –í–ê–ñ–ù–û: ru = —Ä—É—Å—Å–∫–æ–µ headword (—á—Ç–æ–±—ã "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–±—É" –∏ —Ç.–ø. —Ç–æ—á–Ω–æ –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å)
            yield ParsedEntry(hanzi=hanzi, pinyin=pinyin, ru=ru_head)


# =========================
# DB import
# =========================

async def truncate_entries(session: AsyncSession) -> None:
    await session.execute(text("TRUNCATE TABLE public.entries RESTART IDENTITY;"))
    await session.commit()


async def import_file(session: AsyncSession, file_path: Path, batch_size: int = 5000) -> int:
    inserted = 0
    batch: list[Entry] = []

    for item in iter_entries_for_file(file_path):
        hanzi = (item.hanzi or "")[:64]
        pinyin = (item.pinyin[:128] if item.pinyin else None)

        e = Entry(
            hanzi=hanzi if hanzi else "-",
            pinyin=pinyin,
            ru=item.ru,
            pos=None,
            examples=None,
        )
        batch.append(e)

        if len(batch) >= batch_size:
            session.add_all(batch)
            try:
                await session.commit()
                inserted += len(batch)
                print(f"‚úÖ inserted: {inserted}")
            except Exception:
                await session.rollback()
                bad = batch[-1]
                print("‚ùå batch failed near:", bad.hanzi, bad.pinyin, "ru_head:", (bad.ru[:80] if bad.ru else None))
                raise
            finally:
                batch.clear()

    if batch:
        session.add_all(batch)
        try:
            await session.commit()
            inserted += len(batch)
        except Exception:
            await session.rollback()
            bad = batch[-1]
            print("‚ùå final batch failed near:", bad.hanzi, bad.pinyin, "ru_head:", (bad.ru[:80] if bad.ru else None))
            raise

    return inserted


async def main() -> None:
    data_dir = Path("data")
    files: list[Path] = []

    # dabkrs_1..3
    files.extend(sorted(data_dir.glob("dabkrs_*.dsl")))

    # bruks
    bruks = data_dir / "dabruks.dsl"
    if bruks.exists():
        files.append(bruks)

    if not files:
        raise SystemExit("No DSL files found in ./data (expected dabkrs_*.dsl and/or dabruks.dsl)")

    async with AsyncSessionMaker() as session:
        # –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –∫–∞–∂–¥—ã–π —Ä–∞–∑ —Å –Ω—É–ª—è ‚Äî —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π:
        # await truncate_entries(session)

        total = 0
        for fp in files:
            print(f"\n== importing {fp.name} ==")
            count = await import_file(session, fp)
            total += count
            print(f"üéâ finished {fp.name}. inserted: {count}")

        print(f"\nüéâ DONE. Total inserted: {total}")


if __name__ == "__main__":
    asyncio.run(main())